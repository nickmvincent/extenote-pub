---
title: Glossary
visibility: public
type: doc
---

# Glossary

Key terms and concepts used throughout this book.

## A

**Alignment**
: The challenge of ensuring AI systems behave in accordance with human values and intentions. Data-centric alignment focuses on how training data shapes model behavior.

**Attention (Mechanism)**
: The core computational mechanism in transformer models that allows them to weigh the relevance of different parts of the input when generating output.

## B

**Benchmark**
: A standardized test used to evaluate AI model performance. The book discusses how benchmark selection influences which models appear "best."

## C

**Counterfactual**
: A "what if" scenario. Data counterfactuals ask: "What would the model's performance be if trained on different data?"

**Curation**
: The process of selecting, organizing, and refining data for AI training. A form of ranking that determines what data is included or excluded.

## D

**Data Commons**
: A shared pool of data governed collectively for public benefit, as opposed to proprietary data controlled by single entities.

**Data Leverage**
: The collective power of data creators to influence AI development through coordinated action around their data.

**Data Pipeline**
: See *Pipeworks*.

**Data Poisoning**
: Deliberately introducing malicious or misleading data into training sets to compromise model behavior.

**Data Valuation**
: Methods for estimating the contribution of individual data points or datasets to model performance.

## E

**Embedding**
: A dense vector representation of data (text, images, etc.) in a continuous space where similar items are close together.

**Evaluation**
: The process of measuring AI model performance against defined criteria. The book argues evaluation itself involves ranking.

## F

**Fine-tuning**
: Adapting a pre-trained model to a specific task using additional targeted training data.

**Foundation Model**
: A large AI model trained on broad data that can be adapted to many downstream tasks. Examples include GPT, BERT, and LLaMA.

## G

**Generalization**
: An AI model's ability to perform well on data it hasn't seen during training.

**Governance**
: The structures, policies, and processes that determine how AI systems and their data are managed.

## I

**Influence Function**
: A technique for estimating how individual training examples affect model predictions, useful for data valuation and debugging.

## L

**Label**
: Human-provided annotations that tell models what outputs are correct for given inputs. A form of ranking human judgment into training signal.

**Loss Function**
: The mathematical formula that quantifies how wrong a model's predictions are, guiding training toward better performance.

## M

**Memorization**
: When AI models reproduce training data verbatim rather than learning generalizable patterns. Raises privacy and copyright concerns.

**Model Collapse**
: Degradation of model quality when trained on synthetic data generated by other AI models, potentially losing diversity over generations.

## O

**Overfitting**
: When a model learns training data too precisely, including noise, reducing its ability to generalize to new data.

## P

**Pipeworks**
: The interconnected system of data flows, transformations, rankings, and governance structures that move data from creation through AI deployment. A central metaphor in this book.

**Pre-training**
: The initial phase of training a foundation model on large-scale data before fine-tuning for specific tasks.

**Provenance**
: The documented history of dataâ€”its origins, transformations, and chain of custody.

## R

**Ranking**
: The fundamental operation of ordering, selecting, or weighting items. The book argues ranking is central to all AI systems.

**Ranking Mechanism**
: Any process that assigns relative importance, order, or selection to data, features, or outputs.

**RAG (Retrieval-Augmented Generation)**
: A technique combining information retrieval with text generation, where models fetch relevant documents to inform their responses.

**Reinforcement Learning from Human Feedback (RLHF)**
: Training AI models using human preferences as reward signals, used to align language models with human values.

## S

**Scaling Laws**
: Empirical relationships between model size, data quantity, compute, and performance. Guide decisions about AI development investments.

**Shapley Value**
: A game-theoretic concept used to fairly distribute value among contributors, applied in data valuation to estimate each data point's contribution.

**Synthetic Data**
: Data generated by AI systems rather than collected from real-world sources. Can augment training but risks model collapse if overused.

## T

**Training Data**
: The collection of examples used to teach AI models. The book argues training data selection is fundamentally an act of ranking.

**Transfer Learning**
: Using knowledge from one task or domain to improve performance on another, enabled by pre-trained foundation models.

**Transformer**
: The neural network architecture underlying most modern language models, based on attention mechanisms.

## U

**Unlearning**
: Removing the influence of specific training data from a trained model, relevant for privacy rights and copyright compliance.

**Upstream**
: In data pipelines, referring to earlier stages closer to data origins. Upstream interventions affect everything downstream.

## V

**Validation Set**
: Data held out from training to evaluate model performance and tune hyperparameters without biasing the final evaluation.

## W

**Weight (Model)**
: The learned parameters of a neural network that determine how it transforms inputs to outputs.
