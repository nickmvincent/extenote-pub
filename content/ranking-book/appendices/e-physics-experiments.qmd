---
type: book_appendix
title: Experimental Evidence from Physics of Language Models
visibility: public
---
# Experimental Evidence from Physics of Language Models {#sec-appendix-physics}

This appendix examines the "Physics of Language Models" research program (Allen-Zhu, Li, and collaborators) and shows how its controlled experiments provide rigorous empirical grounding for this book's central claims. Their work answers, through systematic experimentation, questions that our framework raises theoretically.

## E.1 The Physics of Language Models Project

### E.1.1 Philosophy and Method

The Physics of Language Models project, led by Zeyuan Allen-Zhu and Yuanzhi Li at Meta FAIR, pursues a distinctive methodology: rather than studying commercial LLMs trained on "messy, secretly preprocessed internet data," they train models on **synthetic datasets** where every variable can be controlled.

Their guiding analogy: just as physicists discovered that iron balls and feathers fall at the same rate only by creating idealized environments (vacuums), the universal laws of language models can only be discovered through controlled experiments where confounds are eliminated.

> "Commercial LLMs are trained on messy, secretly preprocessed internet data. Training LLMs in a controlled, idealized environment allows us to manage the data and tweak hyperparameters—such as data amount, type, difficulty, and format—to scientifically determine factors affecting LLM performance." [@allenzhu2024tutorial]

This methodology complements our framework. Where we ask "what are the governance implications of AI systems that rank information?", they ask "what are the universal laws governing how these systems store, extract, and manipulate information?" Their answers provide empirical validation for claims we advance theoretically.

### E.1.2 Project Structure

The project is organized into parts:

| Part | Focus | Key Question |
|------|-------|--------------|
| Part 1 | Hierarchical Language Structures | How do models learn grammar/syntax? |
| Part 2.1 | Grade-School Math (Hidden Reasoning) | Do models reason or memorize? |
| Part 2.2 | Learning from Mistakes | Can error-correction data improve reasoning? |
| Part 3.1 | Knowledge Storage and Extraction | How is factual knowledge stored? |
| Part 3.2 | Knowledge Manipulation | Can models classify/compare/reason over knowledge? |
| Part 3.3 | Knowledge Capacity Scaling Laws | How much knowledge can models store? |
| Part 4 | Architecture Design | What architectural choices matter? |

For our purposes, **Parts 3.1, 3.2, and 3.3** are most directly relevant—they investigate the relationship between training data and model capabilities that is central to the Data Pipeworks framework.

## E.2 Part 3.1: Knowledge Storage and Extraction

### E.2.1 The Core Experiment

Allen-Zhu and Li created synthetic biography datasets (**bioS** and **bioR**) consisting of ~100,000 fictional individuals with six attributes each: birth date, birth city, university, major, company name, and company city.

Each individual's biography is expressed in six sentences:

> "Anya Briar Forger was born on October 2, 1996."  
> "She spent her early years in Riverside."  
> "She received a degree in Sociology."  
> "She attended the Ivy University of Washington."  
> "She worked at Globex Corporation."  
> "Her company is based in Kyoto."

Models were trained on these biographies, then tested on their ability to answer questions like "Where was Anya Briar Forger born?" or "What did Anya Briar Forger study?"

### E.2.2 The Critical Finding: Augmentation Required

The central result: **knowledge can be memorized but not extracted** unless training data is sufficiently augmented.

Specifically, models trained on biographies with:
- Single entry per person
- Fixed sentence order
- Consistent phrasing

...could reproduce the training data verbatim but **achieved 0% accuracy** on question-answering, even after instruction fine-tuning.

Only when training data included **augmentation**—paraphrasing, sentence shuffling, multiple phrasings per fact—could models reliably extract knowledge.

```
Training Data Type              | QA Accuracy
-------------------------------|------------
Single entry, fixed order       | 0%
Multiple entries (M=4)          | ~50%
+ Sentence permutation          | ~70%
+ Paraphrasing                  | ~85%
+ All augmentations             | ~95%
```

### E.2.3 The Mechanism: Linear Encoding

Using probing techniques, they discovered **why** augmentation matters:

- **Without augmentation**: Knowledge is encoded in distributed, non-linear ways across token embeddings throughout the biography text
- **With augmentation**: Knowledge becomes **linearly encoded** in the hidden embedding of the entity name itself

When you train on "Anya was born in Riverside" plus "Riverside is where Anya was born" plus "Anya's birthplace: Riverside," the model learns to encode "birthplace=Riverside" directly in the representation of "Anya"—making it extractable via simple QA.

### E.2.4 Implications for Our Framework

This finding directly validates several claims:

**For the Data Pipeworks (Chapter 3)**:

The data **structure** at Stage 3 (dataset construction) determines whether knowledge is extractable at Stage 5 (deployment). It's not enough to include information—it must be included in the right form. This elevates dataset curation from preprocessing detail to core AI capability determinant.

**For the Ranking Framework (Chapter 2)**:

The model's ranking over possible answers is determined by how information was structured during training. A fact can be "in" the training data but unretrievable if it wasn't properly augmented—meaning it won't rank highly when queried.

**For Economic Implications (Chapter 6)**:

This suggests that data contributors who provide **diverse formulations** of knowledge (not just single instances) contribute more value. A Wikipedia article that explains a concept multiple ways may be worth more than one that states it once.

**For Evaluation (Chapter 5)**:

Benchmark performance depends not just on whether relevant knowledge was in training data, but on how it was represented. Two models trained on the same facts could have vastly different QA accuracy depending on augmentation.

## E.3 Part 3.2: Knowledge Manipulation

### E.3.1 Beyond Retrieval

Part 3.1 focused on **extraction**: can the model answer "What is X's birthday?" Part 3.2 asks: can models **manipulate** knowledge—classify it, compare entities, perform inverse lookups?

Tasks tested:

| Task | Example |
|------|---------|
| Classification | "Is Anya's birthday before or after January 1, 1990?" |
| Comparison | "Who is older, Anya or Bond?" |
| Inverse search | "Name someone born in Riverside." |

### E.3.2 The Critical Finding: Manipulation Requires Chain-of-Thought

Models that achieved high accuracy on knowledge extraction **failed catastrophically** on manipulation tasks—often near 0% accuracy—unless:

1. **Chain-of-Thought (CoT) was used during training AND inference**, or
2. The model was explicitly trained on manipulation examples

> "Language models excel in knowledge retrieval but struggle even in the simplest classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference." [@allenzhu2024manipulation]

Even GPT-4 and Llama-3 (as of mid-2024) fail on these simple tasks when CoT is not elicited.

### E.3.3 Implications for Our Framework

**For the Ranking Framework**:

This reveals a limitation of the ranking framing: **ranking can retrieve but may not reason**. The model can rank "Riverside" highly when asked about Anya's birthplace, but cannot rank "before 1990" vs "after 1990" correctly when asked to classify—unless trained/prompted to do so explicitly.

The ranking mechanism accurately describes token selection. It does not automatically confer manipulation capabilities. This is important to acknowledge when scoping our claims.

**For the Data Pipeworks**:

Stage 3 interventions matter for manipulation too. If you want models to compare or classify, you need comparison and classification examples in training data—retrieval data alone is insufficient.

**For Counterarguments (Appendix C)**:

This provides nuance for the "emergent capabilities" counterargument. Some capabilities (retrieval) emerge from scale + diverse data. Others (manipulation) require explicit training. The ranking framing remains accurate for both—but predicting which capabilities emerge requires understanding training data composition.

## E.4 Part 3.3: Knowledge Capacity Scaling Laws

### E.4.1 The Question

How much factual knowledge can a language model store? Is there a fundamental limit?

### E.4.2 The Finding: 2 Bits Per Parameter

Through systematic experiments varying model size and knowledge quantity, they establish:

> "Language models can and only can store **2 bits of knowledge per parameter**, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications." [@allenzhu2024scaling]

This is a remarkably clean result. A 7B parameter model can store ~14 billion bits ≈ 1.75 GB of extractable factual knowledge (under optimal conditions).

### E.4.3 Implications for Our Framework

**For the Data Pipeworks**:

Stage 4 (model training) involves **compression**. The 2-bits-per-parameter result quantifies the compression ratio. A model is a lossy compression of its training data, with a now-known theoretical limit.

This has practical implications: if you want a model to "know" more facts, you need more parameters. No amount of clever training can exceed the 2-bit limit.

**For Information Theory Formalism (Appendix B)**:

This result grounds the rate-distortion framework. The 2-bit limit is effectively the rate-distortion bound for factual knowledge storage in transformers.

**For Economic Implications**:

This creates scarcity. A 7B model has finite knowledge capacity. Decisions about **what to include** in training data become consequential—you can't include everything. Dataset curation isn't just preprocessing; it's deciding what the model will and won't know.

## E.5 Part 2.1: Grade-School Math and Hidden Reasoning

### E.5.1 The Question

When models solve math problems (like GSM8K), are they:
- Memorizing templates?
- Performing genuine reasoning?
- Something else?

### E.5.2 Key Findings

Using a synthetic math dataset (**iGSM**) with controllable difficulty, they probed model internals during problem-solving:

**Finding 1: Models develop "reasoning skills," not just templates**

Through careful probing, they show models learn something beyond pattern matching—they develop internal representations that track mathematical relationships during generation.

**Finding 2: The "hidden reasoning process" differs from humans**

Models appear to compute certain intermediate values **before generating them**. Probing reveals the answer is often encoded in hidden states before the reasoning chain is written out.

**Finding 3: Models develop "level-2" skills beyond training distribution**

On some problem types, models generalize beyond their training distribution—suggesting genuine capability acquisition, not just memorization.

**Finding 4: Errors trace to specific "mental processes"**

By probing internal states, they can identify when and where models make mistakes in their hidden reasoning process.

### E.5.3 Implications for Our Framework

**For Counterarguments (Appendix C)**:

This provides nuance for the "emergence" counterargument. Models do develop something like reasoning—but it's still implemented through the ranking mechanism and still depends on training data. The findings don't contradict the ranking framing; they illuminate what ranking can implement when trained appropriately.

**For the "What Ranking Doesn't Explain" Section**:

We acknowledge that ranking describes mechanism, not why outputs are coherent. This work begins to explain the "why"—models develop internal representations that track relevant structure during generation. The ranking mechanism is the substrate; reasoning emerges from training it on appropriate data.

## E.6 Part 2.2: Learning from Mistakes

### E.6.1 The Question

Can models learn to self-correct during generation (not after) by training on error-correction data?

### E.6.2 Key Finding

Yes. When pretraining data includes erroneous solution steps immediately followed by corrections, models learn to:
- Recognize when they're going wrong
- Correct mistakes during generation
- Achieve higher accuracy than models trained on error-free data alone

This is more token-efficient than post-hoc verification (model doesn't generate useless tokens after mistakes).

### E.6.3 Implications for Our Framework

**For the Data Pipeworks**:

This is a Stage 3 intervention. The **format** of training data (error + correction vs. clean only) affects model capabilities at Stage 5. Dataset construction decisions ripple forward.

**For the Ranking Framework**:

Even self-correction operates through ranking. The model learns to rank "wait, let me reconsider" highly when internal states indicate error. It's still token ranking—but ranking informed by learned error-detection patterns.

## E.7 Synthesis: What Physics of LM Validates

### E.7.1 Claims Directly Supported

| Our Claim | Their Evidence |
|-----------|----------------|
| Training data structure determines output quality | Augmentation required for extraction (3.1) |
| Dataset curation is consequential, not mere preprocessing | Same data, different structure → 0% vs 95% accuracy (3.1) |
| Models compress training data | 2 bits per parameter limit (3.3) |
| Ranking describes mechanism, not cognition | Retrieval works; manipulation fails without CoT (3.2) |
| Feedback from training format to capability | Error-correction data improves reasoning (2.2) |

### E.7.2 Claims Nuanced

| Our Claim | Their Nuance |
|-----------|--------------|
| "Ranking over tokens" | Ranking can implement something reasoning-like when trained appropriately (2.1) |
| "Human data dependency" | Models can generalize beyond training distribution on some tasks (2.1) |
| "Output quality tracks training quality" | The relationship is non-linear; augmentation has threshold effects (3.1) |

### E.7.3 Questions Raised

Their work also raises questions our framework should address:

1. **Augmentation economics**: If augmented data is more valuable, how should this affect compensation? Should paraphrasers of existing content receive attribution?

2. **Manipulation training**: If manipulation requires explicit examples, who provides these? What are the labor implications of needing classification/comparison training data?

3. **Capacity constraints**: Given the 2-bit limit, how should we think about "what knowledge deserves to be in the model"? Is this a new form of editorial power?

## E.8 Methodological Alignment

### E.8.1 Complementary Approaches

The Physics of LM project and this book pursue complementary methodologies:

| Aspect | Physics of LM | This Book |
|--------|---------------|-----------|
| Method | Controlled experiments | Theoretical framework + existing evidence |
| Question | "What are the universal laws?" | "What are the governance implications?" |
| Output | Empirical findings | Policy proposals |
| Scope | Model capabilities | Sociotechnical system |

### E.8.2 Why Both Are Needed

Controlled experiments can establish causal relationships but operate in simplified settings. Real-world governance must account for messy, uncontrolled conditions. The Physics of LM findings tell us **what's possible in principle**; our framework asks **what this means in practice**.

Example: They show augmentation enables extraction. We ask: Who does the augmenting? How are they compensated? What happens when augmentation labor is outsourced or automated?

### E.8.3 Using Their Findings

Throughout this book, we draw on their findings as empirical grounding. Specifically:

- **Chapter 2**: Their probing results support the claim that ranking is mechanistically accurate
- **Chapter 3**: Their augmentation findings validate the importance of Stage 3 (datasets)
- **Chapter 4**: Their controlled experiments provide cleaner evidence than observational studies
- **Chapter 5**: Their benchmark critiques align with our evaluation concerns
- **Chapter 6**: Their capacity limits inform economic analysis of data value

## E.9 Limitations and Caveats

### E.9.1 Synthetic vs. Real Data

Their findings come from synthetic datasets. Real-world training data is messier. Some findings may not transfer directly:

- Augmentation thresholds may differ
- Real knowledge has complex interdependencies absent in bioS
- Scale effects may differ at GPT-4 size

They acknowledge this and provide some validation on real models (Llama experiments), but caution is warranted.

### E.9.2 Knowledge vs. Other Capabilities

Their knowledge storage/extraction work focuses on factual knowledge (entity-attribute pairs). Other capabilities—creative writing, code generation, open-ended reasoning—may have different dynamics.

### E.9.3 Architecture Dependence

Some findings may be architecture-specific. They primarily study GPT-2 and Llama architectures. Novel architectures could have different properties.

## E.10 Conclusion

The Physics of Language Models project provides rigorous experimental validation for claims this book advances theoretically. Their findings establish:

1. **Training data structure causally determines model capabilities**—not just training data content
2. **Models are knowledge-limited by capacity constraints**—approximately 2 bits per parameter
3. **Retrieval and manipulation are distinct capabilities**—requiring different training interventions
4. **The ranking mechanism can implement reasoning-like processes**—when trained appropriately

These findings strengthen the case for taking the Data Pipeworks seriously. If a model's capabilities depend sensitively on how training data is structured, then Stage 3 decisions (dataset construction) are among the most consequential choices in AI development. This justifies the governance attention we argue they deserve.

For readers seeking the technical details, the full paper series is available at [physics.allen-zhu.com](https://physics.allen-zhu.com/), with video presentations for each part.

## References for Appendix E

::: {#refs-appendix-e}
:::
