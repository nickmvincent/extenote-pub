---
type: book_chapter
title: Economic Disruption and Power Concentration
visibility: public
---
# Economic Disruption and Power Concentration {#sec-economic-disruption}

## Overview

The ranking framework makes a prediction: if AI systems can rank information better than humans for a given task, those systems can substitute for humans performing that task. This chapter explores the economic implications.

We examine:

1. **A simple model of white-collar work**: Workers as producers of information sequences, competing against alternatives including AI
2. **The Capital Singularity**: Why default dynamics favor concentration of AI capabilities and economic power
3. **The augmentation question**: Why "just make AI that augments rather than substitutes" is harder than it sounds
4. **Industry goals**: What the leading AI organizations say they're building, and what it implies
5. **Countervailing forces**: Why disruption is not inevitable, but depends on governance choices

A note on prediction: Economic forecasting is notoriously unreliable. We make claims about *tendencies* and *dynamics* rather than precise predictions. The goal is to identify forces that require attention, not to forecast specific outcomes.

## Background Research

### The Long History of Automation Anxiety

Concerns about technological unemployment are not new. The Luddite movement of the early 19th century arose when textile workers destroyed machinery that threatened their livelihoods [@binfield2004]. Keynes famously predicted "technological unemployment" as a transition problem in 1930, though he expected it to resolve within a century [@keynes1930]. More recently, Frey and Osborne's influential 2013 analysis predicted 47% of US employment was at high risk of computerization [@frey2017].

This history counsels both attention and humility. Automation has repeatedly displaced workers from specific occupations while total employment has remained robust—the "lump of labor" fallacy suggests work is not fixed in quantity [@autor2015]. Yet past patterns may not predict future outcomes. The relevant question is not whether automation has historically caused permanent unemployment, but whether current AI systems differ in ways that invalidate historical precedent.

### What Makes Generative AI Different?

Several features distinguish generative AI from prior automation waves:

**Generality across domains**: Prior automation technologies typically targeted narrow task categories. Assembly line robots automated specific physical operations; spreadsheet software automated specific calculations. Generative AI systems, trained on diverse text and code, demonstrate capabilities across many domains simultaneously [@bommasani2021]. This breadth may accelerate displacement dynamics by affecting multiple occupations concurrently.

**Cognitive rather than physical tasks**: The automation literature has emphasized that non-routine cognitive tasks remained resistant to automation [@autor2003; @autor2015]. Generative AI challenges this directly. Writing, coding, analysis, and creative work—prototypical non-routine cognitive tasks—are precisely where current systems show greatest capability.

**Rapid capability improvement**: While all technologies improve over time, the pace of LLM capability growth has been remarkable. GPT-4's performance substantially exceeded GPT-3.5 across benchmarks, with only 18 months between releases. If this pace continues, tasks that seem safe from automation today may not remain so.

These differences do not guarantee disruption, but they suggest caution about extrapolating from historical patterns.

### Skill-Biased Technical Change and Its Limits

For decades, economists explained rising wage inequality through "skill-biased technical change" (SBTC): new technologies complement skilled labor while substituting for unskilled labor, increasing the wage premium for education [@acemoglu2011; @card2002]. This framework successfully explained patterns through the 1990s.

However, SBTC faces challenges from recent trends. Wage polarization—growth at both high and low ends with stagnation in the middle—emerged in the 2000s, inconsistent with simple skill-bias predictions [@autor2006]. The "routinization" hypothesis refined SBTC: what matters is not skill per se but whether tasks are routine (and thus automatable) versus non-routine [@autor2003].

Generative AI may require further refinement. Initial evidence suggests that lower-skilled workers sometimes benefit more from AI assistance than higher-skilled workers [@noy2023], potentially narrowing rather than widening inequality within occupations. Whether this pattern persists as systems improve remains unclear.

### Platform Economics and Winner-Take-All Dynamics

The concentration concerns raised in this chapter draw on literature analyzing digital platform economics. Network effects—where a platform's value increases with user numbers—create positive feedback loops favoring market leaders [@shapiro1998]. Data network effects add another layer: more users generate more data, improving the platform's capabilities, attracting more users [@gregory2021].

These dynamics help explain technology industry concentration. Search, social networking, and e-commerce each exhibit strong tendencies toward dominant platforms. The question for AI is whether similar dynamics apply, and whether AI intensifies concentration by adding data-driven capability improvement to existing network effects.

The "Capital Singularity" concept explored in this chapter represents an extreme form of this concern: if AI capabilities improve primarily through data accumulation, and if AI deployment generates data, then organizations with AI advantages may compound those advantages without bound. Whether actual dynamics approach this limiting case is an empirical question, but the theoretical possibility motivates concern.

### Labor Market Institutions and Adjustment

Even optimistic scenarios about automation's aggregate effects acknowledge adjustment costs for displaced workers. The literature on trade-displaced workers [@autor2013] documents persistent earnings losses and geographic concentration of harm. Workers who lose jobs due to automation may face similar challenges—and perhaps worse ones if skills become obsolete rather than merely relocated.

Labor market institutions—unemployment insurance, retraining programs, wage subsidies, collective bargaining—mediate these adjustment processes. Countries with stronger social insurance have historically shown better worker outcomes during technological transitions [@acemoglu2019]. This suggests that policy choices, not just technological trajectories, determine distributional outcomes.

### The Augmentation vs. Substitution Debate

A recurring theme in automation discourse is the distinction between technologies that augment human capabilities versus those that substitute for human labor. Brynjolfsson and McAfee popularized this framing, arguing that the goal should be "racing with the machine" rather than against it [@brynjolfsson2014].

However, the distinction may be less stable than advocates suggest. Many augmenting technologies eventually become substituting technologies as they improve. Word processors augmented typists until they eliminated the typist occupation. GPS augmented taxi drivers until autonomous vehicles threatened the occupation entirely.

More fundamentally, there may be no technical method to ensure augmentation without substitution. If a system can perform a task well enough to augment a human, it can likely perform it well enough to replace the human—the question becomes economic rather than technical. This observation motivates the governance focus in later chapters: if augmentation-only AI is technically infeasible, policy intervention may be necessary to maintain human involvement.

## The Simple Model: Workers as Information Sequence Producers

### What White-Collar Workers Do

Strip away the sociology of work—the meetings, the office politics, the coffee breaks—and focus on the economic function. What do white-collar workers actually produce?

The answer, for most knowledge work, is: **appropriate information sequences for given contexts**.

- The lawyer produces legal documents—sequences of words appropriate to the legal context
- The programmer produces code—sequences of symbols appropriate to the computational context
- The analyst produces reports—sequences of claims appropriate to the decision context
- The marketer produces copy—sequences of phrases appropriate to the persuasion context
- The manager produces emails—sequences of sentences appropriate to the coordination context

This is not a complete description of these jobs. Workers also make judgments, build relationships, and exercise creativity. But the *visible output*, the thing that can be evaluated and compared, is typically an information sequence.

### The Competitive Landscape

At any given time, an employer evaluating whether to continue employing a worker for a sequence-producing task is implicitly comparing that worker to alternatives:

**The domestic labor market.** Could a different domestic worker produce the required sequences at lower cost or higher quality?

**The global labor market.** Could an overseas worker produce the required sequences? Decades of outsourcing and offshoring have made this comparison routine for many tasks.

**Automation and AI.** Could a software system produce the required sequences? This has long been relevant for narrow, routine tasks. With generative AI, it becomes relevant for broad, non-routine tasks.

The ranking framework clarifies what AI needs to do to compete: rank tokens in a way that produces acceptable outputs for the given context. The question is not whether AI can replicate human judgment or understanding in some deep sense. The question is whether it can produce outputs that satisfy the employer's requirements.

### Sociology as Friction

Of course, employment decisions aren't purely about output quality. Bosses like some employees and dislike others. Social obligations create stickiness. Office relationships matter. Organizations resist change. These factors slow substitution dynamics.

But friction is not prevention. The history of offshoring demonstrates that social and cultural preferences give way when cost differentials become large enough. Companies that initially insisted on domestic workers eventually accepted overseas alternatives when competitors forced the issue. Similar dynamics may apply to AI substitution.

The relevant question is how much friction, and how long it lasts. If AI capabilities improve rapidly while friction erodes gradually, substitution proceeds. If capabilities plateau while friction remains strong, substitution stalls. Current trajectories suggest continued capability improvement, which implies continued pressure on human employment in sequence-producing tasks.

### What the Ranking Frame Adds

The ranking framework provides precision to this model. Generative AI systems don't produce outputs through some mysterious intelligence—they rank tokens via softmax and select from the ranking. To substitute for a human, the system needs to rank tokens in a way that produces acceptable outputs.

This has implications:

**Training data determines ranking.** If the training data contains high-quality examples of the relevant output type, the model learns to rank tokens that produce similar outputs highly. For domains with abundant high-quality training data (code, common documents), AI can produce acceptable outputs. For domains with sparse or low-quality training data, it cannot.

**Context length matters.** Tasks requiring extensive context (remembering long conversations, processing large documents) challenge current models. This protects some complex knowledge work—for now.

**Evaluation is key.** If outputs can be quickly evaluated for acceptability, substitution is easier. If outputs require extended evaluation (legal documents that won't be tested in court for years), substitution is harder.

## The Capital Singularity

### The Feedback Loop

Consider the dynamics facing an organization deploying AI at scale:

1. **AI performs tasks.** The organization deploys AI systems for various sequence-producing functions.

2. **Usage generates data.** Every interaction—prompts, responses, user feedback, downstream outcomes—becomes data.

3. **Data improves AI.** The organization uses this data to fine-tune models, identify failure modes, and improve performance.

4. **Better AI performs more tasks.** Improved systems can handle tasks they previously couldn't, expanding the scope of AI deployment.

5. **Repeat.** The cycle continues, with each iteration expanding AI capabilities and the organization's data advantage.

This is a positive feedback loop. Organizations that deploy AI effectively accumulate advantages that enable further effective deployment. The rich get richer.

### From Feedback Loop to Singularity

The "Capital Singularity" is the limiting case of this feedback loop—a scenario where AI advantages compound without bound.

The term deliberately echoes "technological singularity" (a hypothesized point where AI self-improvement escapes human control). But Capital Singularity is an economic concept, not a technical one. It describes the concentration of economic power, not the emergence of superintelligence.

The dynamics are familiar from platform economics. Network effects create positive feedback loops that advantage market leaders. Data network effects intensify these dynamics—more users mean more data, which means better products, which attract more users. AI adds yet another layer—better products can be produced with fewer humans, reducing costs and enabling expansion.

The limiting case—where a single organization captures essentially all AI-related economic value—is probably unrealistic. Competition, regulation, and organizational limits prevent unbounded concentration in practice. But the *direction* of these dynamics is toward concentration, and even partial concentration has significant implications.

### Power Concentration as Default

The important claim is not that Capital Singularity is inevitable but that concentration is the *default trajectory* absent countervailing forces.

Left to market dynamics alone:

- Organizations with AI advantages expand
- Organizations without AI advantages contract
- Data accumulates with the leaders
- Capabilities compound for the leaders
- Market share concentrates

This pattern is already visible in the AI industry. A small number of organizations control frontier models. These organizations have privileged access to compute, data, and talent. Barriers to entry are high and rising.

The analogy to prior technology waves is instructive. Search concentrated into Google. Social networking concentrated into Facebook/Meta. E-commerce concentrated into Amazon. Cloud computing concentrated into a few hyperscalers. These patterns emerged from similar feedback dynamics.

### Why This Matters for Workers

Power concentration has direct implications for labor:

**Reduced bargaining power.** Workers negotiating with monopolistic employers have less leverage than workers in competitive labor markets. If AI capabilities concentrate in few organizations, those organizations can set terms.

**Reduced alternatives.** In concentrated markets, workers displaced from one employer have fewer alternatives. The next employer may be the same organization, a subsidiary, or dependent on the dominant platform.

**Winner-take-all compensation.** Concentrated markets often exhibit winner-take-all compensation patterns—a few superstar workers capture large rewards while most workers see stagnation. AI concentration may intensify this by reducing demand for median workers while maintaining demand for top performers.

The feedback loop doesn't just affect AI companies—it affects anyone whose livelihood depends on labor markets that AI is reshaping.

## The Augmentation Question

### The Appealing Solution

A common response to automation concerns is: "We'll just build AI that augments humans rather than replacing them."

This is appealing. It suggests we can have AI productivity gains without labor displacement. Workers become more productive with AI assistance. Everyone wins.

Various AI leaders have endorsed this framing. Enterprise software companies emphasize "AI copilots" that help workers rather than replace them. Productivity tools are marketed as augmentation, not substitution.

### Why Augmentation-Only Is Hard

But there's a problem: there may be no technical method to build AI systems that can augment but *cannot* substitute.

Consider what augmentation requires. An augmenting AI system must be capable enough to help with a task—to draft text, suggest code, or propose solutions. To be helpful, it must be good at the task.

But a system that's good at a task is a system that could, in principle, perform that task entirely. The augmentation capability and the substitution capability are the same underlying capability, applied differently.

You cannot build a system that can write good first drafts but cannot write good final drafts. You cannot build a system that can suggest good code but cannot write good code independently. The capabilities are not separable at a technical level.

### System-Level Augmentation and Its Fragility

"But we'll keep humans in the loop!" Yes, you can design systems that require human approval before acting. These are augmentation systems—humans remain essential.

The problem is that human-in-the-loop designs face constant economic pressure:

**Humans are expensive.** Every human approval step costs time and money. Organizations face pressure to remove bottlenecks.

**Humans make errors.** Human oversight isn't perfect. If AI makes fewer errors than human reviewers catch, the business case for review weakens.

**Competition doesn't wait.** Organizations that maintain expensive human oversight compete against organizations that remove it. If the latter succeed, the former face pressure to follow.

The history of automation is littered with human-in-the-loop systems that evolved into fully automated systems. Autopilot was designed to assist pilots; now pilots are trained to monitor autopilots. Algorithmic trading began with human approval; now trading occurs faster than humans can follow.

### The Personalization/Substitution Frontier

There's a deeper issue. The capabilities that make AI useful for personalization—learning preferences, adapting to context, anticipating needs—are precisely the capabilities that enable substitution.

A system that knows your writing style well enough to suggest completions knows your writing style well enough to write for you. A system that knows your coding patterns well enough to predict your next line knows your coding patterns well enough to code for you. A system that knows your analytical frameworks well enough to help structure analysis knows your analytical frameworks well enough to perform analysis.

The personalization/substitution frontier is not a stable boundary. As systems become more personalized—more helpful—they become more capable of substitution. Users who want better assistance are training systems to replace them.

## Industry Goal Statements

### What Are They Building?

It's worth examining what the leading AI organizations say they're trying to build.

**OpenAI's Charter** (2018) states the organization's goal as developing "highly autonomous systems that outperform humans at most economically valuable work."

Note the language. Not "systems that help humans with economically valuable work." Not "systems that augment human capabilities." The stated goal is systems that *outperform* humans at work. This is substitution framed as aspiration.

**Sam Altman** (OpenAI CEO) has spoken of building "very powerful systems" that could "do most of the tasks that most people do at their job."

**Dario Amodei** (Anthropic CEO) has discussed scenarios where AI automates large fractions of the economy, while emphasizing safety research alongside capability development.

**Various venture capitalists** have funded companies explicitly targeting automation. "Mechanize venture" and similar initiatives seek to automate specific job categories as investment theses.

### The Implications

These are not hypothetical capabilities that might emerge by accident. They're explicit goals that shape research priorities, hiring, and resource allocation.

When the organizations building AI explicitly aim to outperform humans at "most economically valuable work," it's reasonable to take that goal seriously as a predictor of effort. Whether they succeed is uncertain. That they're trying is documented.

This reframes the augmentation discourse. It's not that augmentation is an alternative goal that just needs more emphasis. The leading organizations have different goals. They're building systems intended to substitute for humans at economically valuable tasks. Augmentation may be a side effect or a transitional state, but it's not the target.

### What This Means for Governance

If substitution is the goal, governance cannot assume augmentation. Policy that hopes AI will naturally remain augmenting is policy based on hope rather than evidence.

This is not an argument against AI development. It's an argument for governance that takes substitution seriously rather than assuming it away. The interventions discussed in later chapters—collective bargaining, data rules, commons protection—are responses to a world where substitution is intended, not an accident to be avoided.

## Why Disruption Requires Intervention

### The Default and the Alternative

The analysis so far suggests a default trajectory:

- AI capabilities continue improving
- Substitution pressure increases across sequence-producing occupations
- Power concentrates with organizations controlling AI capabilities
- Workers face declining bargaining power and fewer alternatives

This default is not inevitable. It's the outcome if current dynamics continue without countervailing forces. The question is what countervailing forces could alter the trajectory.

### Countervailing Force 1: Open-Weight Models

Open-weight models—models whose parameters are freely available—reduce concentration by distributing capabilities. If anyone can run a capable model, AI advantages become harder to monopolize.

The open-source AI movement has made significant progress. Llama, Mistral, and others have released capable models. This creates competitive pressure on closed providers and reduces barriers to entry.

But open-weight models don't solve the concentration problem entirely:

- The largest models require substantial compute to train and run
- Compute is concentrated (a few cloud providers, chip manufacturers)
- Fine-tuning on proprietary data remains a source of advantage
- Open models can be captured through dominant platforms

Open-weight models shift but don't eliminate concentration dynamics.

### Countervailing Force 2: Competitive Dynamics

Competition among AI providers could prevent monopolization. If multiple organizations offer capable systems, no single organization dominates.

Currently, competition is genuine. Multiple frontier labs compete. Enterprise customers have choices. Prices are falling.

But competitive dynamics can evolve toward concentration. History suggests that platform markets often tip toward single winners. Whether AI markets follow this pattern is an open question.

### Countervailing Force 3: Antitrust

Antitrust enforcement could prevent monopolization. Regulators could block mergers, investigate anti-competitive practices, and require interoperability.

Antitrust has shown renewed vigor in the technology sector. The FTC and DOJ have filed significant cases against major platforms. European regulators have been more aggressive still.

But antitrust faces challenges:

- Legal standards developed for industrial-era markets may not fit AI
- Proving harm requires understanding AI economics
- Enforcement is slow relative to technology development
- Global coordination is difficult

Antitrust is a useful tool but not sufficient on its own.

### Countervailing Force 4: Collective Bargaining for Information

This brings us to the central proposal of this book: collective bargaining over data and information.

The logic is straightforward. AI systems depend on training data. Training data originates from human activity. If the humans whose activity generates training data can bargain collectively, they gain leverage over AI development.

This is the subject of the next two chapters. Here we note the economic logic:

- Workers cannot easily bargain over wages when employers can substitute AI
- But AI depends on data that originated from workers (and others)
- Data is an earlier point in the production chain
- Collective organization around data may restore bargaining power

The idea is not to prevent AI development but to ensure that those whose contributions enable AI share in its benefits. This requires moving the bargaining point upstream—from employment to data.

## Summary

The ranking framework explains economic disruption: if AI can rank information better than humans for a task, it can substitute for humans on that task.

**White-collar workers produce information sequences.** They compete against domestic workers, global workers, and increasingly, AI systems. Sociology creates friction but not prevention.

**Power concentration is the default trajectory.** Feedback loops between AI deployment, data accumulation, and capability improvement favor incumbent AI organizations. Without intervention, concentration proceeds.

**Augmentation-only AI is technically infeasible.** Systems capable enough to augment are capable enough to substitute. Human-in-the-loop designs face economic pressure. The leading AI organizations explicitly aim at substitution, not just augmentation.

**Countervailing forces exist but are insufficient alone.** Open-weight models, competition, and antitrust provide some counterweight. But the central intervention this book proposes is collective bargaining over information—the subject of the next chapter.

The key insight: the disruption isn't purely about AI capabilities. It's about who controls those capabilities and on what terms. Technology determines what's possible; governance determines what happens. Without governance intervention, the default trajectory favors concentration and displacement. With appropriate intervention, alternatives are possible.
