---
type: bibtex_entry
citation_key: lewis2020rag
entry_type: inproceedings
title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
authors:
  - Patrick Lewis
  - Ethan Perez
  - Aleksandra Piktus
  - Fabio Petroni
  - Vladimir Karpukhin
  - Naman Goyal
  - Heinrich Küttler
  - Mike Lewis
  - Wen-tau Yih
  - Tim Rocktäschel
  - Sebastian Riedel
  - Douwe Kiela
year: '2020'
venue: NeurIPS 2020
url: https://arxiv.org/abs/2005.11401
doi: 10.5555/3495724.3496517
visibility: public
abstract: >-
  Large pre-trained language models have been shown to store factual knowledge
  in their parameters, and achieve state-of-the-art results when fine-tuned on
  downstream NLP tasks. However, their ability to access and precisely manipulate
  knowledge is still limited, and hence on knowledge-intensive tasks, their
  performance lags behind task-specific architectures. Additionally, providing
  provenance for their decisions and updating their world knowledge remain open
  research problems. Pre-trained models with a differentiable access mechanism
  to explicit non-parametric memory can overcome this issue, but have so far
  been only investigated for extractive downstream tasks. We explore a
  general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) —
  models which combine pre-trained parametric and non-parametric memory for
  language generation.
tags:
  - ml-methods
  - language-models
  - retrieval
  - knowledge-intensive
cited_in:
  - ranking-book
---

