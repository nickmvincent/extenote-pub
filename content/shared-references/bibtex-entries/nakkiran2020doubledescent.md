---
title: 'Deep Double Descent: Where Bigger Models and More Data Hurt'
type: bibtex_entry
citation_key: nakkiran2020doubledescent
entry_type: inproceedings
authors:
  - Preetum Nakkiran
  - Gal Kaplun
  - Yamini Bansal
  - Tristan Yang
  - Boaz Barak
  - Ilya Sutskever
year: '2020'
venue: ICLR 2020
url: 'https://arxiv.org/abs/1912.02292'
abstract: >-
  Demonstrates that double descent occurs across model size, training epochs,
  and dataset size in modern deep networks. Introduces effective model
  complexity to unify these phenomena and shows regimes where more data hurts.
tags:
  - language-models
  - ml-methods
  - training-dynamics
visibility: public
check_log:
  checked_at: '2025-12-30T15:40:29.202Z'
  checked_with: auto
  status: not_found
---

