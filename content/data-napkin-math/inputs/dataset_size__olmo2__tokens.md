---
title: Total pre-training tokens (Olmo 2)
value: 5000000000000
scale: 1000000000
display_units: billions of tokens
variable_name: dataset_size__olmo2__tokens
variable_type: dataset_size
entity: olmo2
units: tokens
source_url: 'https://huggingface.co/allenai/OLMo-2-1124-7B'
date_added: '2025-03-19T00:00:00.000Z'
tags:
  - variable-type:dataset-size
  - entity:olmo2
  - unit:tokens
visibility: public
type: InputVariable
---

# Total pre-training tokens (Olmo 2)

**Value:** 5,000 billions of tokens

## Description

Total tokens used to pre-training a model

## Key Assumption

...

## Source

- [https://huggingface.co/allenai/OLMo-2-1124-7B](https://huggingface.co/allenai/OLMo-2-1124-7B)
- Source is the Olmo 2 model card. It describes total number of tokens for pre-training.
